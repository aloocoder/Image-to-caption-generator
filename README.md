## ğŸ–¼ï¸ Image Caption Generator using Flickr8k Dataset

This project builds an **Image Caption Generator** using deep learning techniques and the **Flickr8k** dataset. The core objective is to generate meaningful captions for a given image by combining **Computer Vision** (for feature extraction) and **Natural Language Processing** (for language modeling).

### ğŸ” Overview

The model is trained on the Flickr8k dataset, which contains 8,000 images, each with 5 corresponding human-written captions. The model architecture uses a combination of **CNNs (VGG16)** for image feature extraction and **LSTM** layers for generating natural language captions.


### ğŸ§° Technologies & Tools
- **Python**
- **TensorFlow / Keras**
- **NumPy / Pandas**
- **Matplotlib**
- **VGG16 (pre-trained model)**
- **LSTM (for sequence modeling)**
- **Tokenizer (Keras)**


### ğŸ“¦ Dataset
- **Images:** Sourced from the Flickr8k dataset.
- **Captions:** Cleaned and preprocessed text associated with each image.
- Directory: `Flickr8k_Dataset` and `Flickr8k_text`.

### ğŸ“Œ Workflow Summary

#### 1. ğŸ“ **Image Feature Extraction**
- Uses the **VGG16** model pre-trained on ImageNet.
- The last classification layer is removed.
- Images are passed through the model to extract feature vectors (4096-dim).

#### 2. ğŸ“ **Captions Preprocessing**
- Captions are loaded and cleaned.
- Start (`<start>`) and end (`<end>`) tokens are added to each caption.
- Mapped with corresponding image IDs.

#### 3. ğŸ§  **Model Architecture**
- **Image Features â†’ Dense Layer**
- **Captions â†’ Embedding Layer â†’ LSTM**
- Both branches are merged using the `add` layer and passed through Dense layers.
- Final output predicts the next word in the caption sequence.

#### 4. ğŸ§ª **Training**
- Maximum caption length and vocabulary size are computed.
- Tokenizer is fitted on all captions.
- Data generator is used for efficient training on image-text pairs.
- Model is trained using `categorical_crossentropy`.

#### 5. ğŸ”® **Inference**
- At test time, images are passed to the trained model.
- The model generates one word at a time until the `<end>` token is predicted or max length is reached.


### ğŸ“ˆ Results
- The model is capable of generating syntactically correct and contextually relevant captions.
- Example:
  - Input: ğŸ¶ (dog running in field)
  - Output: `"a dog is running through the grass"`


### ğŸš€ Future Improvements
- Use more advanced models like **InceptionV3**, **ResNet**, or **Transformer-based** models (e.g., ViT + GPT).
- Incorporate **BLEU Score** or **METEOR** for quantitative evaluation.
- Add attention mechanism for better alignment between image regions and words.

